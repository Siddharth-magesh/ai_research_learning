training:
  epochs: 10
  batch_size: 32
  gradient_accumulation_steps: 1

  optimizer:
    type: adamw
    lr: 3e-4
    betas: [0.9, 0.98]
    eps: 1e-8
    weight_decay: 0.01

  scheduler:
    type: linear_warmup
    warmup_steps: 4000

  loss:
    type: cross_entropy
    ignore_index: -100

  mixed_precision: true
  gradient_clipping: 1.0

  checkpointing:
    save_dir: ./transformers/checkpoints/
    save_best_only: true